{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_blank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkswndms4782/BoostCamp_AI_Tech/blob/main/Assignment1_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UL2tS-EQZKN"
      },
      "source": [
        "# Assignment 1 : Classification & Fine-tuning\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_aO83c8nKZk"
      },
      "source": [
        "# Seed\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "torch.manual_seed(0)\r\n",
        "torch.cuda.manual_seed(0)\r\n",
        "np.random.seed(0)\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "# Ignore warnings\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nCnWRyQh2P"
      },
      "source": [
        "### **1.1 VGG-11 Implementation**\r\n",
        "\r\n",
        "아래 Table은 여러 종류의 VGG 네트워크에 대한 각각의 구성을 나타낸 것입니다.\r\n",
        "<br>A에서 E까지 다양한 크기의 VGG 중에서 구현하고자 하는 네트워크는 **A에 해당하는 VGG-11**입니다.\r\n",
        "\r\n",
        "----\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?id=1bFKnmwcbdJLQdCjNPxQdaYnTNCmBt0rK'  width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NUH5K20jl2m"
      },
      "source": [
        "아래의 코드는 VGG-11의 네트워크를 나타내는 클래스입니다. 위의 Table을 참고하여 2개의 **TO DO** 과제를 채워주세요 :)\n",
        "\n",
        "- **TO DO (1)** : Feature extractor 역할을 수행하는 convolution layers를 구현하는 것이 과제입니다. 주어진 ```self.conv1, self.bn, self.pool1```을 참고하여 나머지 부분을 채워주세요.\n",
        "\n",
        "- **TO DO (2)** : Convolution filter로 이루어진 feature extractor에서 얻은 이미지에 대한 feature를 통해 classification task를 수행하는 fully connected layer를 구현하고자 합니다. 주어진 ```self.fc1, self.dropout2```를 참고하여 나머지 부분을 채워주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiK_5kEYQsda"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class VGG11(nn.Module):\r\n",
        "  def __init__(self, num_classes=1000):\r\n",
        "    super(VGG11, self).__init__()\r\n",
        "\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    \r\n",
        "    # Convolution Feature Extraction Part\r\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\r\n",
        "    self.bn1   = nn.BatchNorm2d(64)\r\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    '''==========================================================='''\r\n",
        "    '''======================== TO DO (1) ========================'''\r\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n",
        "    self.bn2   = nn.BatchNorm2d(128)\r\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\r\n",
        "    self.bn3_1   = nn.BatchNorm2d(256)\r\n",
        "    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\r\n",
        "    self.bn3_2   = nn.BatchNorm2d(256)\r\n",
        "    self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn4_1   = nn.BatchNorm2d(512)\r\n",
        "    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn4_2   = nn.BatchNorm2d(512)\r\n",
        "    self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn5_1   = nn.BatchNorm2d(512)\r\n",
        "    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn5_2   = nn.BatchNorm2d(512)\r\n",
        "    self.pool5   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "    '''======================== TO DO (1) ========================'''\r\n",
        "    '''==========================================================='''\r\n",
        "    \r\n",
        "\r\n",
        "    # Fully Connected Classifier Part\r\n",
        "    self.fc1      = nn.Linear(512 * 7 * 7, 4096)\r\n",
        "    self.dropout1 = nn.Dropout()\r\n",
        "    \r\n",
        "    '''==========================================================='''\r\n",
        "    '''======================== TO DO (2) ========================'''\r\n",
        "    self.fc2      = nn.Linear(4096, 4096)\r\n",
        "    self.dropout2 = nn.Dropout()\r\n",
        "    \r\n",
        "    self.fc3      = nn.Linear(4096, 1000)\r\n",
        "    '''======================== TO DO (2) ========================'''\r\n",
        "    '''==========================================================='''\r\n",
        "    \r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    # Convolution Feature Extraction Part\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.bn1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool1(x)\r\n",
        "\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.bn2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool2(x)\r\n",
        "\r\n",
        "    x = self.conv3_1(x)\r\n",
        "    x = self.bn3_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv3_2(x)\r\n",
        "    x = self.bn3_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool3(x)\r\n",
        "\r\n",
        "    x = self.conv4_1(x)\r\n",
        "    x = self.bn4_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv4_2(x)\r\n",
        "    x = self.bn4_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool4(x)\r\n",
        "\r\n",
        "    x = self.conv5_1(x)\r\n",
        "    x = self.bn5_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv5_2(x)\r\n",
        "    x = self.bn5_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool5(x)\r\n",
        "\r\n",
        "    # Fully Connected Classifier Part\r\n",
        "    x = torch.flatten(x, 1)\r\n",
        "    x = self.fc1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.dropout1(x)\r\n",
        "    \r\n",
        "    x = self.fc2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.dropout2(x)\r\n",
        "    \r\n",
        "    x = self.fc3(x)\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6WDaKSQkw9m"
      },
      "source": [
        "----\n",
        "구현한 VGG-11에 문제가 없다면 네트워크의 **output tensor는 1000-dim vector**에 해당해야 합니다. \n",
        "\n",
        "<br>아래의 코드를 실행하여 **output tensor의 shape을 확인**해봅시다.\n",
        "<br> 만약 ```out = model(x)```를 실행하였을 때 오류가 발생한다면 layer 구현에 오류가 있는 것이니 다시 한번 네트워크 코드에 문제가 없는지 확인해보세요 :)\n",
        "\n",
        "\n",
        "아래의 문구와 동일하게 출력된다면 output tensor의 shape이 알맞게 return된 것입니다!\n",
        "```python\n",
        "\"Output tensor shape is : torch.Size([1, 1000])\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FugiHIBFISi_"
      },
      "source": [
        "# Network\r\n",
        "model = VGG11(num_classes=1000)\r\n",
        "\r\n",
        "# Random input\r\n",
        "x = torch.randn((1, 3, 224, 224))\r\n",
        "\r\n",
        "# Forward\r\n",
        "out = model(x)\r\n",
        "\r\n",
        "# Check the output shape\r\n",
        "print(\"Output tensor shape is :\", out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LqmANrdm3CJ"
      },
      "source": [
        "----\r\n",
        "아래는 네트워크 구현이 정확하게 되었는지를 확인하기 위한 코드입니다.\r\n",
        "\r\n",
        "<br>아래의 코드는 **VGG-11의 중간 feature map들의 shape**을 바탕으로 일련의 연산을 수행하여 **하나의 값**을 계산합니다.\r\n",
        "<br>채점을 위하여 아래의 코드 결과로 얻은 값을 **edwith에 제출**해주세요 :)\r\n",
        "\r\n",
        "(주의 : 정확한 채점을 위하여 아래 코드는 수정하지 마세요!)\r\n",
        "\r\n",
        "<br>예를 들어, 아래와 같은 실행 결과를 얻으셨다면 edwith 퀴즈에 7777을 선택해주세요.\r\n",
        "```python\r\n",
        "\"Your answer is : 7777\"\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohvN7vZGij9i"
      },
      "source": [
        "import base64, copy\r\n",
        "\r\n",
        "class Calculator:\r\n",
        "  '''\r\n",
        "  NOTE : DO NOT MODIFY THE CODE BELOW.\r\n",
        "  '''\r\n",
        "  def __init__(self, model):\r\n",
        "    self.answer = 0\r\n",
        "    layers = [b'Y29udjNfMQ==\\n', b'cG9vbDM=\\n', b'Y29udjVfMg==\\n']\r\n",
        "    for l in layers:\r\n",
        "      self.hook = model._modules[base64.decodebytes(l).decode()].register_forward_hook(self.hook_fn)\r\n",
        "    \r\n",
        "  def hook_fn(self, module, input, output):\r\n",
        "    self.answer += self._get_answer(output)\r\n",
        "  \r\n",
        "  def _get_answer(self, l):\r\n",
        "    _, A, B, C = l.shape\r\n",
        "    return A*(B-C//3)\r\n",
        "    \r\n",
        "  def unregister_forward_hook(self):\r\n",
        "    self.hook.remove()\r\n",
        "  \r\n",
        "\r\n",
        "def calc_anwser(model):\r\n",
        "  # NOTE : DO NOT MODIFY THE CODE BELOW.\r\n",
        "  model_test = copy.deepcopy(model)\r\n",
        "  ans_calculator = Calculator(model_test)\r\n",
        "\r\n",
        "  x = torch.rand(1,3,224,224)\r\n",
        "  model_test(x)\r\n",
        "\r\n",
        "  print(\"Your answer is : %d\" % ans_calculator.answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJnv-DFDoUhj"
      },
      "source": [
        "calc_anwser(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk6iqOpYCPgs"
      },
      "source": [
        "----\r\n",
        "### **1.2 Fine Tuning**\r\n",
        "\r\n",
        "Fine tuning을 적용하였을 때의 장점을 확인하고자 하는 과제입니다.\r\n",
        "\r\n",
        "네트워크를 처음부터 학습(scratch training) 하였을 때와 fine tuning을 적용하였을 때를 비교하는 것이 과제의 핵심입니다.\r\n",
        "\r\n",
        "Pytorch에서 제공하는 ```torchvision.models.vgg11```의 ImageNet에 대한 ```pre-trained``` 옵션을 통해 과제를 수행해봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckZBLzmhkZDY"
      },
      "source": [
        "#### **>>> Dataset**\r\n",
        "\r\n",
        "해당 과제에서는 이미지 속 인물의 **마스크 착용 여부와 마스크 착용 방식을 분류**하는 네트워크를 학습시키고자 합니다. (학습 시간 단축을 위해 일부 데이터만 사용)\r\n",
        "\r\n",
        "시간 상의 문제로 실제로는 훨씬 많은 iteration을 반복하여 학습을 진행하여 정확도를 높여야 하지만 해당 과제에서는 1 epoch에 해당하는 **120번의 iteration만 진행**할 예정입니다. 과제 제출을 마치신 다음에는 더욱 오랜 시간동안 학습을 진행하여 성능을 더 높여보세요!\r\n",
        "\r\n",
        "과제를 수행하기 앞서 별도로 전달해드린 **데이터셋 활용 가이드**를 따라 **```APY201231001_Face_Recognition_Masks_Dataset_Shared_Subset_1050.zip```** 압축 파일을 구글 드라이브에 **바로가기 추가**해주세요 :)\r\n",
        "\r\n",
        "\r\n",
        "<br></br>**주의!** 해당 과정은 **데이터 저작권 보호**를 위해 로컬로 직접 데이터를 <U>**다운로드 받는 것을 금지**</U>하기 때문입니다. 또한 교육이 종료된 이후에 해당 데이터셋을 **구글 드라이브에서 파기**할 것을 원칙으로 합니다.\r\n",
        "<img src='https://drive.google.com/uc?id=10H0twn3TQLneZtukSThviNLdFgDnJCiu'  width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFBV7FShE4q-"
      },
      "source": [
        "# Mount the google drive to access the dataset.\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJPUtMwhnQLk"
      },
      "source": [
        "# 저장하신 압축 파일의 경로에 맞게 아래의 압축 해제 명령어를 수정해주세요\r\n",
        "# 해당 과정은 약 10분 정도 소요됩니다\r\n",
        "\r\n",
        "#!unzip /content/gdrive/MyDrive/APY201231001_Face_Recognition_Masks_Dataset_Shared_Subset_1050.zip -d /content/gdrive/MyDrive/FaceDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChDPHjqgFE_3"
      },
      "source": [
        "# Hyper-parameters\r\n",
        "data_root = '/content/gdrive/MyDrive/FaceDataset/data'\r\n",
        "log_dir   = '/content/gdrive/MyDrive/FaceDataset/log'\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "lr = 1e-4\r\n",
        "input_size = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ov3bo_Ojrjd"
      },
      "source": [
        "# Dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "class MaskDataset(Dataset):\n",
        "  def __init__(self, data_root, is_Train=True, input_size=224, transform=None):\n",
        "    super(MaskDataset, self).__init__()\n",
        "\n",
        "    self.img_list = self._load_img_list(data_root, is_Train)\n",
        "    self.len = len(self.img_list)\n",
        "    self.input_size = input_size\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_path = self.img_list[index]\n",
        "    \n",
        "    # Image Loading\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img/255.\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    # Ground Truth\n",
        "    label = self._get_class_idx_from_img_name(img_path)\n",
        "\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def _load_img_list(self, data_root, is_Train):\n",
        "    # Change the name of directory which has inconsistent naming rule.\n",
        "    full_img_list = glob(data_root + '/*')\n",
        "    for dir in full_img_list:\n",
        "      dirname = os.path.basename(dir)\n",
        "      if '-1' in dirname:\n",
        "        os.rename(dir, dir.replace(dirname, dirname.replace('-1', '1')))\n",
        "    \n",
        "    # ID < 1000 for Training (N=721)\n",
        "    # 1000 < ID < 1050 for Validation (N=63)\n",
        "    img_list = []\n",
        "    for dir in glob(data_root + '/*'):\n",
        "      if is_Train and (self._load_img_ID(dir) < 500):\n",
        "        img_list.extend(glob(dir+'/*'))\n",
        "      elif not is_Train and (1000 < self._load_img_ID(dir) < 1050):\n",
        "        img_list.extend(glob(dir+'/*'))\n",
        "\n",
        "    return img_list\n",
        "\n",
        "  def _load_img_ID(self, img_path):\n",
        "    return int(os.path.basename(img_path).split('_')[0])\n",
        "\n",
        "  def _get_class_idx_from_img_name(self, img_path):\n",
        "    img_name = os.path.basename(img_path)\n",
        "\n",
        "    if 'normal' in img_name: return 0\n",
        "    elif 'mask1' in img_name: return 1\n",
        "    elif 'mask2' in img_name: return 2\n",
        "    elif 'mask3' in img_name: return 3\n",
        "    elif 'mask4' in img_name: return 4\n",
        "    elif 'mask5' in img_name: return 5\n",
        "    elif 'incorrect_mask' in img_name: return 6\n",
        "    else:\n",
        "      raise ValueError(\"%s is not a valid filename. Please change the name of %s.\" % (img_name, img_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXw95POHx_WH"
      },
      "source": [
        "# Dataset and Data Loader\r\n",
        "transform = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Resize((224,224)),\r\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                          std=[0.229, 0.224, 0.225])\r\n",
        "])\r\n",
        "\r\n",
        "train_dataset = MaskDataset(data_root, is_Train=True, input_size=input_size, transform=transform)\r\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\r\n",
        "\r\n",
        "valid_dataset = MaskDataset(data_root, is_Train=False, input_size=input_size, transform=transform)\r\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FAPE0A52iCv"
      },
      "source": [
        "# Misc\r\n",
        "\r\n",
        "class AverageMeter(object):\r\n",
        "  \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "      self.reset()\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    self.val = 0\r\n",
        "    self.avg = 0\r\n",
        "    self.sum = 0\r\n",
        "    self.count = 0\r\n",
        "\r\n",
        "  def update(self, val, n=1):\r\n",
        "    self.val = val\r\n",
        "    self.sum += val * n\r\n",
        "    self.count += n\r\n",
        "    self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHvjw_Y1lv9"
      },
      "source": [
        "----\r\n",
        "#### **>>> 1.2.1 Training From Scratch**\r\n",
        "\r\n",
        "Pre-training을 사용하지 않고 초기 상태부터 학습을 진행(from scratch)하는 단계입니다.\r\n",
        "\r\n",
        "- **TO DO Setting** : 매우 간단하지만 어떻게 보면 핵심이라고 할 수 있는 설정입니다. ```torchvision.models.vgg11```을 이용하여 네트워크를 선언할 때, ImageNet에 pre-train된 weights를 초기값으로 설정할지 여부를 결정하는 ```pretrained``` 파라미터 값을 ```True```와 ```False``` 둘 중 하나의 값으로 채워주세요.\r\n",
        "\r\n",
        "- **TO DO Main (1)** : VGG-11을 본격적으로 학습하는 과정입니다. 주석에 적힌 내용을 따라 loss function인 ```criterion```과 ```optimizer```를 활용하여 빈 부분을 채워주세요.\r\n",
        "\r\n",
        "- **TO DO Main (2)** : 학습된 VGG-11을 validation dataset에 대해 평가하는 과정입니다. Validation 과정에서는 <U>gradient 계산과 backpropagation이 필요 없다</U>는 것에 주목하여 빈 부분을 채워주세요.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJpRE9lQ1hjI"
      },
      "source": [
        "from torchvision.models import vgg11\r\n",
        "\r\n",
        "'''======================== TO DO Setting ========================'''\r\n",
        "pretrained =      False\r\n",
        "'''==============================================================='''\r\n",
        "\r\n",
        "model = vgg11(pretrained)\r\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=7, bias=True)\r\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M0JEGaJ1OwI"
      },
      "source": [
        "# Loss function and Optimizer\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yJKV0bbDNhn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "766d55e3-2574-4366-cb4e-0f88c8bc3632"
      },
      "source": [
        "# Main\r\n",
        "os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "with open(os.path.join(log_dir, 'scratch_train_log.csv'), 'w') as log:\r\n",
        "  # Training\r\n",
        "  for iter, (img, label) in enumerate(train_loader):\r\n",
        "    '''================================================================'''\r\n",
        "    '''======================== TO DO Main (1) ========================'''\r\n",
        "    # optimizer에 저장된 미분값을 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "    img, label = img.to(\"cuda\", torch.float), label.to(\"cuda\", torch.long)\r\n",
        "    # img, label = img.float().cuda(), label.float().cuda()\r\n",
        "    # img, label = torch.tensor(img, dtype = torch.float, device = \"cuda\"), torch.tensor(label, dtype = torch.long, device = \"cuda\")\r\n",
        "\r\n",
        "    # 모델에 이미지 forward\r\n",
        "    pred_logit = model.forward(img)\r\n",
        "\r\n",
        "    # loss 값 계산\r\n",
        "    loss = criterion(pred_logit, label)\r\n",
        "\r\n",
        "    # Backpropagation\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # Accuracy 계산\r\n",
        "    pred_label = torch.argmax(pred_logit, dim=1)\r\n",
        "    acc = sum(label == pred_label) / len(img) \r\n",
        "    '''======================== TO DO Main (1) ========================'''\r\n",
        "    '''================================================================'''\r\n",
        "\r\n",
        "    train_loss = loss.item()\r\n",
        "    train_acc = acc\r\n",
        "\r\n",
        "    # Validation for every 20 epoch\r\n",
        "    if (iter % 20 == 0) or (iter == len(train_loader)-1):\r\n",
        "      valid_loss, valid_acc = AverageMeter(), AverageMeter()\r\n",
        "\r\n",
        "      for img, label in valid_loader:\r\n",
        "        '''================================================================'''\r\n",
        "        '''======================== TO DO Main (2) ========================'''\r\n",
        "        # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "        img, label = img.to(\"cuda\", torch.float), label.to(\"cuda\", torch.long)  \r\n",
        "\r\n",
        "        # 모델에 이미지 forward (gradient 계산 X)\r\n",
        "        pred_logit = model.forward(img)\r\n",
        "        print(pred_logit)\r\n",
        "        \r\n",
        "\r\n",
        "        # loss 값 계산\r\n",
        "        loss = criterion(pred_logit, label)\r\n",
        "\r\n",
        "        # Accuracy 계산\r\n",
        "        pred_label = torch.argmax(pred_logit, dim=1)\r\n",
        "        acc = sum(label == pred_label) / len(img) \r\n",
        "        '''======================== TO DO Main (2) ========================'''\r\n",
        "        '''================================================================'''\r\n",
        "\r\n",
        "        valid_loss.update(loss.item(), len(img))\r\n",
        "        valid_acc.update(acc, len(img))\r\n",
        "\r\n",
        "      valid_loss = valid_loss.avg\r\n",
        "      valid_acc = valid_acc.avg\r\n",
        "\r\n",
        "      print(\"Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f\" %\r\n",
        "            (iter, len(train_loader), train_loss, train_acc, valid_loss, valid_acc))\r\n",
        "      \r\n",
        "      # Train Log Writing\r\n",
        "      log.write('%d,%.4f,%.4f,%.4f,%.4f\\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-4.7780,  0.8862, -1.0204,  0.9456,  0.4854, -2.9474, -0.3289],\n",
            "        [-6.5830,  6.9724, -5.8522,  0.9046,  0.7913, -1.5344, -1.0534],\n",
            "        [-3.9251, -0.0479, -1.1967, -0.1323, -0.2585, -3.9164,  4.2602],\n",
            "        [-1.8229,  0.5716, -4.2606,  1.7722,  1.3517, -1.7685,  0.1268],\n",
            "        [-5.6214,  3.1155, -2.9713,  3.5791,  1.5936, -3.2512,  0.4924],\n",
            "        [ 5.0278, -1.2031, -3.8252, -1.1119,  0.3559, -1.9871,  1.5483],\n",
            "        [-4.4051,  1.2737, -2.5924,  4.7696,  1.2512, -2.8861, -0.5536],\n",
            "        [-4.5271, -1.7526,  1.3057,  0.6279, -0.2100, -1.3879,  3.4717]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "tensor([[-4.0142,  0.3467, -0.9201,  2.2316,  0.8014, -1.9822, -0.5338],\n",
            "        [-2.5285,  6.1970, -4.1559,  1.2591, -1.1698, -3.0803,  1.0658],\n",
            "        [-2.9271,  0.0752, -2.6554,  2.7166,  1.8421, -1.2452,  0.7590],\n",
            "        [-4.8287, -0.7320,  2.8399, -0.1818, -1.0811, -1.3855, -0.3799],\n",
            "        [ 5.7951, -1.9123, -3.9603, -1.3634,  0.0898, -2.7163,  0.8038],\n",
            "        [-5.0175,  0.5836, -3.0976,  6.1083,  0.3184, -3.5031, -1.8116],\n",
            "        [-6.5014,  0.9761,  2.8448,  0.4872, -0.3902,  0.1716,  1.4523],\n",
            "        [-7.3607,  5.2001, -2.8569,  1.1624,  0.0433, -0.8484, -1.3986]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-f0d31c51a8fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;34m'''================================================================'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m'''======================== TO DO Main (2) ========================'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-64fea9cfe389>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Image Loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlsd7Hgz1zDq"
      },
      "source": [
        "----\r\n",
        "#### **>>> 1.2.2 Fine Tuning**\r\n",
        "\r\n",
        "ImageNet에 pre-train된 weights를 초기값으로 사용하여 학습을 진행(**fine tuning**)하는 단계입니다.\r\n",
        "<br>이번 과제에서는 feature extraction 역할을 하는 **convolution layer 부분의 weights를 고정(freeze)**시키고 나머지 classifier 역할을 담당하는 **fully connected 부분만 학습**할 예정입니다.\r\n",
        "\r\n",
        "- **TO DO Setting** : 마찬가지로 ImageNet에 pre-train된 weights를 초기값으로 설정할지 여부를 결정하는 ```pretrained``` 파라미터 값을 ```True```와 ```False``` 둘 중 하나의 값으로 채워주세요.\r\n",
        "\r\n",
        "- **TO DO Main (1) & (2)** : 1.2.1 Training From Scratch에서 작성하신 코드를 그대로 사용하시면 됩니다 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7580AoNDtFv"
      },
      "source": [
        "from torchvision.models import vgg11\r\n",
        "\r\n",
        "'''======================== TO DO Setting ========================'''\r\n",
        "pretrained =      True\r\n",
        "'''==============================================================='''\r\n",
        "\r\n",
        "model = vgg11(pretrained)\r\n",
        "model.classifier[6] = nn.Linear(in_features=4096, out_features=7, bias=True)\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "# Freeze the feature extracting convolution layers\r\n",
        "for param in model.features.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiEqoF2TKkeg"
      },
      "source": [
        "# Loss function and Optimizer\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPkDbF47Kkeg"
      },
      "source": [
        "# Main\r\n",
        "os.makedirs(log_dir, exist_ok=True)\r\n",
        "\r\n",
        "with open(os.path.join(log_dir, 'fine_tuned_train_log.csv'), 'w') as log:\r\n",
        "  # Training\r\n",
        "  for iter, (img, label) in enumerate(train_loader):\r\n",
        "    '''================================================================'''\r\n",
        "    '''======================== TO DO Main (1) ========================'''\r\n",
        "    # optimizer에 저장된 미분값을 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "    img, label = img.to(\"cuda\", torch.float), label.to(\"cuda\", torch.long)\r\n",
        "\r\n",
        "    # 모델에 이미지 forward\r\n",
        "    pred_logit = model.forward(img)\r\n",
        "\r\n",
        "    # loss 값 계산\r\n",
        "    loss = criterion(pred_logit, label)\r\n",
        "\r\n",
        "    # Backpropagation\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "\r\n",
        "    # Accuracy 계산\r\n",
        "    pred_label = torch.argmax(pred_logit, dim = 1)\r\n",
        "    acc = sum(label == pred_label) / len(img)\r\n",
        "    '''======================== TO DO Main (1) ========================'''\r\n",
        "    '''================================================================'''\r\n",
        "\r\n",
        "    train_loss = loss.item()\r\n",
        "    train_acc = acc\r\n",
        "\r\n",
        "    # Validation for every 20 epoch\r\n",
        "    if (iter % 20 == 0) or (iter == len(train_loader)-1):\r\n",
        "      valid_loss, valid_acc = AverageMeter(), AverageMeter()\r\n",
        "\r\n",
        "      for img, label in valid_loader:\r\n",
        "        '''================================================================'''\r\n",
        "        '''======================== TO DO Main (2) ========================'''\r\n",
        "        # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\r\n",
        "        img, label = img.to(\"cuda\", torch.float), label.to(\"cuda\", torch.long)\r\n",
        "\r\n",
        "        # 모델에 이미지 forward (gradient 계산 X)\r\n",
        "        pred_logit = model.forward(img)\r\n",
        "        \r\n",
        "\r\n",
        "        # loss 값 계산\r\n",
        "        loss = criterion(pred_logit, label)\r\n",
        "\r\n",
        "        # Accuracy 계산\r\n",
        "        pred_label = torch.argmax(pred_logit, dim = 1)\r\n",
        "        acc = sum(label == pred_label) / len(img)\r\n",
        "        '''======================== TO DO Main (2) ========================'''\r\n",
        "        '''================================================================'''\r\n",
        "\r\n",
        "        valid_loss.update(loss.item(), len(img))\r\n",
        "        valid_acc.update(acc, len(img))\r\n",
        "\r\n",
        "      valid_loss = valid_loss.avg\r\n",
        "      valid_acc = valid_acc.avg\r\n",
        "\r\n",
        "      print(\"Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f\" %\r\n",
        "            (iter, len(train_loader), train_loss, train_acc, valid_loss, valid_acc))\r\n",
        "      \r\n",
        "      # Train Log Writing\r\n",
        "      log.write('%d,%.4f,%.4f,%.4f,%.4f\\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV14IsbuYRNj"
      },
      "source": [
        "#### **>>> 1.2.3 Training Log Visualization**\r\n",
        "\r\n",
        "초기부터 학습(training from scratch)한 모델과 fine tuning을 사용한 모델을 비교하기 위해 **각 epoch 마다 저장한 validation 데이터셋에 대한 loss 값과 accuracy를 시각화**하여 확인하고자 합니다.\r\n",
        "\r\n",
        "- **TO DO** : 아래의 시각화 코드를 활용하여 초기부터 학습하였을 때와 fine tuning을 진행하였을 때의 성능 변화를 시각화해주세요. ```matplotlib```을 이용하여 시각화한 그래프가 해당 colab notebook 파일에 남아있어야 하며 해당 과정을 마치신 뒤에 edwith의 댓글로 colab link를 남겨주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oil0TZpXBk0"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD6C0AUdXTtV"
      },
      "source": [
        "# Load log file\r\n",
        "scratch_train_log = pd.read_csv(os.path.join(log_dir, 'scratch_train_log.csv'), index_col=0, header=None)\r\n",
        "fine_tuned_train_log = pd.read_csv(os.path.join(log_dir, 'fine_tuned_train_log.csv'), index_col=0, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ssR4yaPP-A"
      },
      "source": [
        "# Visualize training log\r\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))\r\n",
        "\r\n",
        "ax1.plot(scratch_train_log.iloc[:,0], label='Scratch Training')\r\n",
        "ax1.plot(fine_tuned_train_log.iloc[:,0], label='Fine Tuning')\r\n",
        "ax1.set_title('Training Loss Graph', fontsize=15)\r\n",
        "ax1.set_xlabel('Iteration', fontsize=15)\r\n",
        "ax1.set_ylabel('Loss', fontsize=15)\r\n",
        "\r\n",
        "fig.legend(fontsize=15)\r\n",
        "\r\n",
        "ax2.plot(scratch_train_log.iloc[:,1], label='Scratch Training')\r\n",
        "ax2.plot(fine_tuned_train_log.iloc[:,1], label='Fine Tuning')\r\n",
        "ax2.set_title('Training Accuracy Graph', fontsize=15)\r\n",
        "ax2.set_xlabel('Iteration', fontsize=15)\r\n",
        "ax2.set_ylabel('Accuracy', fontsize=15)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbEUOGmxPQQd"
      },
      "source": [
        "# Visualize validation log\r\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))\r\n",
        "\r\n",
        "ax1.plot(scratch_train_log.iloc[:,2], label='Scratch Training')\r\n",
        "ax1.plot(fine_tuned_train_log.iloc[:,2], label='Fine Tuning')\r\n",
        "ax1.set_title('Validation Loss Graph', fontsize=15)\r\n",
        "ax1.set_xlabel('Iteration', fontsize=15)\r\n",
        "ax1.set_ylabel('Loss', fontsize=15)\r\n",
        "\r\n",
        "fig.legend(fontsize=15)\r\n",
        "\r\n",
        "ax2.plot(scratch_train_log.iloc[:,3], label='Scratch Training')\r\n",
        "ax2.plot(fine_tuned_train_log.iloc[:,3], label='Fine Tuning')\r\n",
        "ax2.set_title('Validation Accuracy Graph', fontsize=15)\r\n",
        "ax2.set_xlabel('Iteration', fontsize=15)\r\n",
        "ax2.set_ylabel('Accuracy', fontsize=15)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgk-V2_4FHQH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}