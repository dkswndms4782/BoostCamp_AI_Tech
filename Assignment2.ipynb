{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkswndms4782/BoostCamp_AI_Tech/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZgD64bgxIB"
      },
      "source": [
        "# Assignment 2 : Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWiCBlveQUGU"
      },
      "source": [
        "이번 과제에서는, Assignment 1에서 classification 문제를 풀기 위해 구성했던 CNN 모델을 **segmentation 문제를 풀기 위한 모델**로 재구성해보는 과정을 진행합니다. 모델에서 layer들을 구성하는 모듈들이 동작하는 방식을 더 잘 이해하는 시간이 되셨으면 좋겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD0o1Jvg0TRQ"
      },
      "source": [
        "## Note"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sksbtbMiyjrQ"
      },
      "source": [
        "### Note on copying Tensor\r\n",
        "\r\n",
        "You may use different methods for copying Tensor. Here, we list possible means you may use.\r\n",
        "\r\n",
        "(1) y = x.detach()    \r\n",
        "- This creates new Tensor that does not compute gradient from the original Tensor \r\n",
        "- You can modify such method **x.clone().detach().requires_grad_(True)** to indicate requires_grad option\r\n",
        "\r\n",
        "(2) y = x.data\r\n",
        "- x.data  used to get Tensor from Variable \r\n",
        "- **detach()** may be better, since Tensor copied from **.data** method even uses wrong gradient during update (while **detach()** outputs error on wrong gradients) \r\n",
        "\r\n",
        "(3) y = torch.empty_like(x).copy_(x)\r\n",
        "- This creates new Tensor that has the same content from the original Tensor\r\n",
        "\r\n",
        "(4) y = torch.tensor(x)\r\n",
        "\r\n",
        "(5) y = tensor.new_tensor(x)\r\n",
        "- You can indicates **require_grad** option with this method\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMP-YaG5z-Us"
      },
      "source": [
        "### Note on assigning weight\r\n",
        "\r\n",
        "You may use different methods for assignin weight from pre-defined Tensor. Here, we list possible means you may use.\r\n",
        "\r\n",
        "\r\n",
        "(1) layer_.weight = torch.nn.Parameter(tensor_)\r\n",
        "\r\n",
        "(2) layer_.wiehgt = torch.Tensor(tensor_)\r\n",
        "\t- You can use this methods during initionalization\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PUeyDQ1Ypc_"
      },
      "source": [
        "##**Google Drive Mount**\r\n",
        "해당 문제를 풀기 위해서는 (1) pretrained model 과 (2) 저희가 선별한 inference용도의 data가 따로 필요합니다. 과제 폴더에 해당 데이터들을 넣어드렸는데요. \"드라이브에 바로가기 추가\" 옵션을 활용하여 내 드라이브로 바로가기를 추가하신 후, 아래의 코드들을 돌려서 colab 작업 환경에 파일들을 잘 copy 하시기 바랍니다.\r\n",
        "\r\n",
        "    \r\n",
        "***Inference를 위해 사용하는 data들에는 저작권 문제가 있으니, 공지된 저작권 관련 policy를 잘 따라주셔야 합니다.***\r\n",
        "\r\n",
        "    \r\n",
        "혹시라도 cp가 제대로 안되는 경우 모델/데이터 로딩이 안되실 겁니다. 그럴 땐, 밑에 cp command의 source path를 알맞게 고쳐주시면 됩니다. 가령, 과제 폴더 전체를 바로가기로 추가하실 경우 source path를 ./gdrive/Mydrive/과제2/Assignment2_model_pretrinaed.pth 등으로 고쳐주시면 됩니다.\r\n",
        "\r\n",
        "!cp <U>[source path]</U> [destination path]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro31tWH-6C1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbd8721-e6e8-49e5-cca3-21ffa66f442b"
      },
      "source": [
        "# Mount the google drive to access the dataset.\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e7OTY9qYxi8"
      },
      "source": [
        "!cp ./gdrive/MyDrive/과제2/Assignment2_model_pretrained.pth ./model.pth"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRlASBXjY80v"
      },
      "source": [
        "!cp -R ./gdrive/MyDrive/과제2/Assignment2_data ./data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLqwf7Gig5h6"
      },
      "source": [
        "## **VGG-11 Implementation**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64T_QHtRumr"
      },
      "source": [
        "Let's convert VGG-11 model for image classification into semantic segmentation. You will modify the last fc layer into 1x1 conv. layer. Recall that semantic segmentation can be considered as a classification problem on each pixel of the input image data.\r\n",
        "\r\n",
        "Here, we defined backbone model that is commonly used both for classification and segmentation tasks. Backbone of the model is composed of 5 blocks of convolutional layers (along with batch-normalization and pooling layers).\r\n",
        "\r\n",
        "For classification, you may use fc layer for the last layer right before prediction, as seen in VGG11 (in assignment 1.1). However, this last fc layer for classification may not be appropriate for solving segmentation problem, as spatial information is lost in fc layer. In order to preserve such information, you may use convolutional layer for the last part; this idea can be seen in papers like [1].\r\n",
        "\r\n",
        "\r\n",
        "[[1] Long et al, Fully Convolutional Networks for Semantic Segmentation, CVPR 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6BvmhgM5xDq"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKRxeso3uz4g"
      },
      "source": [
        "The code below implements customed-VGG11 model composed of backbone and appropraite modules. Please fill in  2 **TO DO** and check the following answers with **calc_answer** functions below.\r\n",
        "\r\n",
        "\r\n",
        "- **TO DO (1)** : You should build a layer for the semantic segmentation. (Hint: Take a look at the **forward** method)\r\n",
        "\r\n",
        "- **TO DO (2)** : You should reshape & copy the parameter of **fc_out** layer of model for classification into defining layer (in Todo 1).\r\n",
        "\r\n",
        "- **Hint** : You may use the functions below to fill in Todo's:    \r\n",
        "(1) torch.reshape    \r\n",
        "(2) torch.nn.Conv2d    \r\n",
        "(3) torch.nn.Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcgayKEpvjXA"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class VGG11BackBone(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(VGG11BackBone, self).__init__()\r\n",
        "\r\n",
        "    self.relu = nn.ReLU(inplace=True)\r\n",
        "    \r\n",
        "    # Convolution Feature Extraction Part\r\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\r\n",
        "    self.bn1   = nn.BatchNorm2d(64)\r\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n",
        "    self.bn2   = nn.BatchNorm2d(128)\r\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\r\n",
        "    self.bn3_1   = nn.BatchNorm2d(256)\r\n",
        "    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\r\n",
        "    self.bn3_2   = nn.BatchNorm2d(256)\r\n",
        "    self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn4_1   = nn.BatchNorm2d(512)\r\n",
        "    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn4_2   = nn.BatchNorm2d(512)\r\n",
        "    self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "\r\n",
        "    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn5_1   = nn.BatchNorm2d(512)\r\n",
        "    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\r\n",
        "    self.bn5_2   = nn.BatchNorm2d(512)\r\n",
        "  \r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.bn1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool1(x)\r\n",
        "\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.bn2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool2(x)\r\n",
        "\r\n",
        "    x = self.conv3_1(x)\r\n",
        "    x = self.bn3_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv3_2(x)\r\n",
        "    x = self.bn3_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool3(x)\r\n",
        "\r\n",
        "    x = self.conv4_1(x)\r\n",
        "    x = self.bn4_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv4_2(x)\r\n",
        "    x = self.bn4_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.pool4(x)\r\n",
        "\r\n",
        "    x = self.conv5_1(x)\r\n",
        "    x = self.bn5_1(x)\r\n",
        "    x = self.relu(x)\r\n",
        "    x = self.conv5_2(x)\r\n",
        "    x = self.bn5_2(x)\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6p9JUoARoKE"
      },
      "source": [
        "class VGG11Classification(nn.Module):\r\n",
        "  def __init__(self, num_classes = 7):\r\n",
        "    super(VGG11Classification, self).__init__()\r\n",
        "\r\n",
        "    self.backbone = VGG11BackBone()\r\n",
        "    self.pool5   = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
        "    self.gap      = nn.AdaptiveAvgPool2d(1)\r\n",
        "    self.fc_out   = nn.Linear(512, num_classes)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.backbone(x)\r\n",
        "    x = self.pool5(x)\r\n",
        "\r\n",
        "    x = self.gap(x)\r\n",
        "    x = torch.flatten(x, 1)\r\n",
        "    x = self.fc_out(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxFQGZ8GRprs"
      },
      "source": [
        "class VGG11Segmentation(nn.Module):\r\n",
        "  def __init__(self, num_classes = 7):\r\n",
        "    super(VGG11Segmentation, self).__init__()\r\n",
        "\r\n",
        "    self.backbone = VGG11BackBone()\r\n",
        "    \r\n",
        "    '''==========================================================='''\r\n",
        "    '''======================== TO DO (1) ========================'''\r\n",
        "    self.conv_out = torch.nn.Conv2d(512, 7, kernel_size=3, padding=1)\r\n",
        "\r\n",
        "#(1) torch.reshape\r\n",
        "#(2) torch.nn.Conv2d\r\n",
        "#(3) torch.nn.Parameter\r\n",
        "#x.clone().detach().requires_grad_(True) \r\n",
        "    '''======================== TO DO (1) ========================'''\r\n",
        "    '''==========================================================='''\r\n",
        "    self.upsample = torch.nn.Upsample(scale_factor=16, mode='bilinear', align_corners=False)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.backbone(x)\r\n",
        "    x = self.conv_out(x)\r\n",
        "    x = self.upsample(x)\r\n",
        "    assert x.shape == (1, 7, 224, 224)\r\n",
        "\r\n",
        "    return x\r\n",
        "\r\n",
        "  def copy_last_layer(self, fc_out):\r\n",
        "    \"\"\"\r\n",
        "    Copy last pre-trained fully-connected layer of classification model\r\n",
        "    into self.conv_out layer of segmentation model in appropriate shape\r\n",
        "\r\n",
        "    Keyword arguments:\r\n",
        "    fc_out: the fc layer of classification model (with shape of (7, 512))\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    '''==========================================================='''\r\n",
        "    '''======================== TO DO (2) ========================'''\r\n",
        "    self.conv_out.weight = torch.nn.Parameter(fc_out.weight.reshape(7,512,1,1))\r\n",
        "    self.conv_out.bias = torch.nn.Parameter(fc_out.bias)\r\n",
        "    \r\n",
        "    '''======================== TO DO (2) ========================'''\r\n",
        "    '''==========================================================='''\r\n",
        "    assert self.conv_out.weight[0][0] == fc_out.weight[0][0]\r\n",
        "    \r\n",
        "    return "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyhZWsW4wQhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574508d3-7cdb-40ff-8061-0136f7301740"
      },
      "source": [
        "test_input = torch.randn((1, 3, 224, 224))\r\n",
        "\r\n",
        "modelC = VGG11Classification()\r\n",
        "out = modelC(test_input)\r\n",
        "print(out.shape)\r\n",
        "\r\n",
        "modelS = VGG11Segmentation()\r\n",
        "out = modelS(test_input)\r\n",
        "print(out.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7])\n",
            "torch.Size([1, 7, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sP7AuEr5u_s"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFWdI0s5XPed"
      },
      "source": [
        "This dataloader is slightly modified from assignment1.1, since we do not use large train/valid data, but only few images for inference on segmentation problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF4PQFwprGH6"
      },
      "source": [
        "# Dataset\r\n",
        "import torch\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from glob import glob\r\n",
        "\r\n",
        "class MaskDataset(Dataset):\r\n",
        "  def __init__(self, data_root, input_size=224, transform=None):\r\n",
        "    super(MaskDataset, self).__init__()\r\n",
        "\r\n",
        "    self.img_list = glob(os.path.join(data_root, '*.jpg'))\r\n",
        "    self.len = len(self.img_list)\r\n",
        "    self.input_size = input_size\r\n",
        "    self.transform = transform\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    img_path = self.img_list[index]\r\n",
        "  \r\n",
        "    # Image Loading\r\n",
        "    img = cv2.imread(img_path)\r\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
        "    img = img/255.\r\n",
        "\r\n",
        "    if self.transform:\r\n",
        "      img = self.transform(img)\r\n",
        "\r\n",
        "    # Ground Truth\r\n",
        "    label = self._get_class_idx_from_img_name(img_path)\r\n",
        "\r\n",
        "    return img, label\r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n",
        "  \r\n",
        "  def _get_class_idx_from_img_name(self, img_path):\r\n",
        "    img_name = os.path.basename(img_path)\r\n",
        "\r\n",
        "    if 'normal' in img_name:\r\n",
        "      return 0\r\n",
        "    elif 'mask1' in img_name:\r\n",
        "      return 1\r\n",
        "    elif 'mask2' in img_name:\r\n",
        "      return 2\r\n",
        "    elif 'mask3' in img_name:\r\n",
        "      return 3\r\n",
        "    elif 'mask4' in img_name:\r\n",
        "      return 4\r\n",
        "    elif 'mask5' in img_name:\r\n",
        "      return 5\r\n",
        "    elif 'incorrect_mask' in img_name:\r\n",
        "      return 6\r\n",
        "    else:\r\n",
        "      raise ValueError(\"%s is not a valid filename. Please change the name of %s.\" % (img_name, img_path))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq1bul_R5y0m"
      },
      "source": [
        "## Weight Reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mX5Nlq7DCNJ"
      },
      "source": [
        "### Loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saYj35mV51bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634f01ac-f160-4658-dff1-95cd2cc02461"
      },
      "source": [
        "## Model Loading\r\n",
        "model_root = './model.pth'\r\n",
        "\r\n",
        "modelC = VGG11Classification()\r\n",
        "modelC.load_state_dict(torch.load(model_root))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_5h7m-xDIl1"
      },
      "source": [
        "### Copy weight to reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhy-045BDQMo"
      },
      "source": [
        "Here, we copy the last layer of pretrained model for classification task into new model for segmentation task using 1x1 fully-convolutional layer. In order to fit into the new model, the weight should be reshaped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka9YYwGDfP-f"
      },
      "source": [
        "## Copy Weight\n",
        "modelS  = VGG11Segmentation()\n",
        "modelS.backbone = modelC.backbone\n",
        "\n",
        "w_fc = modelC.fc_out\n",
        "modelS.copy_last_layer(w_fc)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg0UVYC8DxN9"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqCeTPr9ZjIm"
      },
      "source": [
        "You may test on your segmentation model with prepared data with below cells.\r\n",
        "\r\n",
        "The below figure is an example result. You may check that the model is focusing on the part around the mask in the image, so can extract mask segmentation.\r\n",
        "\r\n",
        "<img src='https://drive.google.com/uc?id=1IFw0QT2zbr1txEQXaTBGuRGtgXBm8ruP'  width=\"224\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbPFipcVD2QU"
      },
      "source": [
        "# Image Loading\r\n",
        "data_root = './data'\r\n",
        "input_size = 224\r\n",
        "transform = transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Resize((224,224)),\r\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                          std=[0.229, 0.224, 0.225])\r\n",
        "])\r\n",
        "batch_size = 1\r\n",
        "\r\n",
        "test_dataset = MaskDataset(data_root, input_size=input_size, transform=transform)\r\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMDFZG9Fq2E",
        "outputId": "c6c08a3b-6b43-4d96-b34d-864530df9a49"
      },
      "source": [
        "for iter, (img, label) in enumerate(test_loader):\n",
        "  print(img.size())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmhj45b-gUCK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "05aae660-5863-43c9-93ae-4143854d52d8"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "# Test on Segmentation\n",
        "modelS.cuda().float()\n",
        "modelS.eval()\n",
        "\n",
        "for iter, (img, label) in enumerate(test_loader):\n",
        "  img = img.float().cuda()\n",
        "  # Inference for Semantic Segmentation\n",
        "  res = modelS(img)[0]\n",
        "\n",
        "  heat = res[label[0]]\n",
        "  resH = heat.cpu().detach().numpy()\n",
        "  heatR, heatC = np.where(resH > np.percentile(resH, 95))\n",
        "  \n",
        "  seg = torch.argmax(res, dim=0)\n",
        "  seg = seg.cpu().detach().numpy()\n",
        "  [segR, segC] = np.where(seg == np.int(label[0].cpu()))\n",
        "\n",
        "  resS = np.zeros((224,224))\n",
        "  for i, r in enumerate(heatR):\n",
        "    c = heatC[i]\n",
        "    if (r in segR) and (c in segC):\n",
        "      resS[r,c] = 1\n",
        "  \n",
        "  want_to_check_heat_map_result = False\n",
        "\n",
        "  # Plot segmentation result\n",
        "  matplotlib.pyplot.imshow(img[0].cpu().permute(1, 2, 0))\n",
        "  if want_to_check_heat_map_result:\n",
        "    matplotlib.pyplot.imshow(resH, cmap='jet', alpha=0.3)\n",
        "  matplotlib.pyplot.imshow(resS, alpha=0.4)\n",
        "  matplotlib.pyplot.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in method size of Tensor object at 0x7fc1da25dcd0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-803ec7bb128c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Inference for Semantic Segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mheat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4414a0982fba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEBoeEzFwkzf"
      },
      "source": [
        "#### **Disccusion**\r\n",
        "\r\n",
        "You may also check heatmap result with **want_to_check_heat_map_result** flag. This might be helpful to notice why model is showing such results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8XJttjpgtbg"
      },
      "source": [
        "## Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPOs_td-j_y3"
      },
      "source": [
        "The cells below implements simple function that compute the answer for the problem you will be submit on edwith. With proper code filled, you will get an integer answer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQIbMhn_gvRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e83ccd26-2b33-41f8-bc8f-acb1a97fb13f"
      },
      "source": [
        "def calc_answer_2_1(model):\r\n",
        "  \"\"\"\r\n",
        "  Compute the size of conv_out layer in modified model for solving semantic segmentation task\r\n",
        "  \"\"\"\r\n",
        "  answer  = 0\r\n",
        "\r\n",
        "  size = model.conv_out.weight.size()\r\n",
        "  for s in size:\r\n",
        "    answer += s\r\n",
        "  \r\n",
        "  return answer\r\n",
        "\r\n",
        "\r\n",
        "print(f\"The answer for problem (1) is {calc_answer_2_1(modelS)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer for problem (1) is 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IH4GXGghoEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "feb935b8-84f1-4eb2-99a0-8d5810b0a46c"
      },
      "source": [
        "def calc_answer_2_2(model, dataloader):\r\n",
        "  \"\"\"\r\n",
        "  Compute the overall sum of data in row of segmentation result, from input image data with label of 6\r\n",
        "  \"\"\"\r\n",
        "  answer = 0\r\n",
        "\r\n",
        "  for iter, (img, label) in enumerate(test_loader):\r\n",
        "    if label == 6:\r\n",
        "      img = img.float().cuda()\r\n",
        "      res = modelS(img)[0]\r\n",
        "\r\n",
        "      heat = res[label[0]]\r\n",
        "      resH = heat.cpu().detach().numpy()\r\n",
        "      heatR, heatC = np.where(resH > np.percentile(resH, 95))\r\n",
        "      \r\n",
        "      seg = torch.argmax(res, dim=0)\r\n",
        "      seg = seg.cpu().detach().numpy()\r\n",
        "      [segR, segC] = np.where(seg == np.int(label[0].cpu()))\r\n",
        "\r\n",
        "      resS = np.zeros((224,224))\r\n",
        "      for i, r in enumerate(heatR):\r\n",
        "        c = heatC[i]\r\n",
        "        if (r in segR) and (c in segC):\r\n",
        "          answer += r\r\n",
        "\r\n",
        "  return answer\r\n",
        "\r\n",
        "print(f\"The answer for problem (2) is {calc_answer_2_2(modelS, test_loader)}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-94e2f4e74dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The answer for problem (2) is {calc_answer_2_2(modelS, test_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-94e2f4e74dbb>\u001b[0m in \u001b[0;36mcalc_answer_2_2\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mheat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4414a0982fba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d224f1e0b78>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yubv0uUvuV0k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}