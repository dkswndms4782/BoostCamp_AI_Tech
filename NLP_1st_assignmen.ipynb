{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP 1st assignmen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkswndms4782/BoostCamp_AI_Tech/blob/main/NLP_1st_assignmen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1c3kfN0nuq-"
      },
      "source": [
        "# 1. Spacy를 이용한 영어 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_HUiKMUAd8"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JzKRDIBUEfd"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2aruQ5dYMBM"
      },
      "source": [
        "### 1.1 Tokenezation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNVS9mSTUJ7L"
      },
      "source": [
        "text = nlp('Naver Connect and Upstage Boostcamp')\n",
        "print ([token.text for token in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ykKX1BUUI1"
      },
      "source": [
        "doc = nlp('This assignment is about Natural Language Processing.' 'In this assignment, we will do preprocessing')\n",
        "print ([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xlhp12rV34q"
      },
      "source": [
        "text=nlp(\"The film's development began when Marvel Studios received a loan from Merrill Lynch in April 2005. After the success of the film Iron Man in May 2008, \\\n",
        "Marvel announced that The Avengers would be released in July 2011 and would bring together Tony Stark, Steve Rogers, Bruce Banner, and Thor from Marvel's previous films. \\\n",
        "With the signing of Johansson as Natasha Romanoff in March 2009, the film was pushed back for a 2012 release. Whedon was brought on board in April 2010 and rewrote the original screenplay by Zak Penn. Production began in April 2011 in Albuquerque, \\\n",
        "New Mexico, before moving to Cleveland, Ohio in August and New York City in September. The film has more than 2,200 visual effects shots.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfKx_fOZYyfp"
      },
      "source": [
        "### 1.2 불용어 (Stopword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No59QbQyVZ2k"
      },
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "for stop_word in list(spacy_stopwords)[:30]:\n",
        "  print(stop_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuAshJZ_Vfj-"
      },
      "source": [
        "stopword_text = [token for token in text if not token.is_stop]\n",
        "print(stopword_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Xoxy6SY9m8"
      },
      "source": [
        "### 1.3 Lemmatization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPDccv8yW9jS"
      },
      "source": [
        "for token in text[:20]:\n",
        "  print (token, \"-\", token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QU2ZNFDZhjC"
      },
      "source": [
        "### 1.4 그외 token class의 attributes \n",
        "\n",
        "https://spacy.io/api/token#attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6wAEJBQXPiK"
      },
      "source": [
        "print(\"token \\t is_punct \\t is_space \\t shape_ \\t is_stop\")\n",
        "print(\"=\"*70)\n",
        "for token in text[21:31]:\n",
        "  print(token,\"\\t\", token.is_punct, \"\\t\\t\",token.is_space,\"\\t\\t\", token.shape_, \"\\t\\t\",token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saa9trANZhKI"
      },
      "source": [
        "## 빈칸완성 과제1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-IdgoiZVoJ"
      },
      "source": [
        "def is_token_allowed(token):\n",
        "# stopword와 punctutation을 제거해주세요.\n",
        "\n",
        "  #if문을 작성해주세요.\n",
        "  \n",
        "  ##TODO#\n",
        "  if  token.is_punct or token.is_stop:\n",
        "  ##TODO##\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def preprocess_token(token):\n",
        "  #lemmatization을 실행하는 부분입니다. \n",
        "  return token.lemma_.strip().lower()\n",
        "\n",
        "filtered_tokens = [preprocess_token(token) for token in text if is_token_allowed(token)]\n",
        "answer=['film', 'development','begin', 'marvel','studios', 'receive','loan', 'merrill','lynch', 'april','2005', 'success','film', 'iron','man', '2008','marvel','announce', 'avengers','release', 'july','2011', 'bring','tony', 'stark','steve', 'rogers','bruce', 'banner','thor', 'marvel','previous', 'film','signing', 'johansson','natasha','romanoff','march','2009','film','push','2012','release','whedon','bring','board','april','2010','rewrote','original','screenplay','zak','penn','production','begin','april','2011','albuquerque','new','mexico','move','cleveland','ohio','august','new','york','city','september','film','2,200','visual','effect','shot']\n",
        "assert filtered_tokens == answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htdYW5Gcc_Ii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv92V7lFnpYt"
      },
      "source": [
        "# 2. 한국어 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWdfTvNnoZaX"
      },
      "source": [
        "### 2.1 Mecab를 이용한 형태소 분석 기반 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UHPPZuBdBkN"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF_-GhMpcVfM"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "import operator\n",
        "tokenizer = Mecab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaYnMm8xdzdK"
      },
      "source": [
        "text=\"최강의 슈퍼히어로들이 모였다! 지구의 운명을 건 거대한 전쟁이 시작된다! 지구의 안보가 위협당하는 위기의 상황에서 슈퍼히어로들을 불러모아 세상을 구하는, 일명 어벤져스 작전. 에너지원 테서랙트를 이용한 적의 등장으로 인류가 위험에 처하자 국제평화유지기구인 쉴드의 국장 닉 퓨리는 어벤져스 작전을 위해 전 세계에 흩어져 있던 슈퍼히어로들을 찾아나선다. 아이언맨부터 토르, 헐크, 캡틴 아메리카는 물론, 쉴드의 요원인 블랙 위도우, 호크아이까지, 최고의 슈퍼히어로들이 어벤져스의 멤버로 모이게 되지만, 각기 개성이 강한 이들의 만남은 예상치 못한 방향으로 흘러가는데… 지구의 운명을 건 거대한 전쟁 앞에 어벤져스 작전은 성공할 수 있을까?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9uP6cPCestY"
      },
      "source": [
        "print(tokenizer.morphs(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wBJySBoev7d"
      },
      "source": [
        "stopwords=['의','가','이','은','다','들','을','는','인','위해','과','던','도','를','로','게','으로','까지','자','에','을까','는데','치','와','한','하다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3akksDdvf4QH"
      },
      "source": [
        "tokenized_text = [word for word in tokenizer.morphs(text) if not word in stopwords] # 불용어 제거\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeQZoMEbkuQG"
      },
      "source": [
        "### 2.2 음절 단위 토크나이징 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTtNysPffaEl"
      },
      "source": [
        "starry_night=['계절이 지나가는 하늘에는',\n",
        "'가을로 가득 차 있습니다.',\n",
        "'나는 아무 걱정도 없이',\n",
        "'가을 속의 별들을 다 헬 듯합니다.',\n",
        "'가슴 속에 하나 둘 새겨지는 별을',\n",
        "'이제 다 못 헤는 것은',\n",
        "'쉬이 아침이 오는 까닭이요,',\n",
        "'내일 밤이 남은 까닭이요,',\n",
        "'아직 나의 청춘이 다하지 않은 까닭입니다.',\n",
        "'별 하나에 추억과',\n",
        "'별 하나에 사랑과',\n",
        "'별 하나에 쓸쓸함과',\n",
        "'별 하나에 동경과',\n",
        "'별 하나에 시와',\n",
        "'별 하나에 어머니, 어머니,',\n",
        "\"어머님, 나는 별 하나에 아름다운 말 한마디씩 불러 봅니다. 소학교 때 책상을 같이 했던 아이들의 이름과, 패, 경, 옥, 이런 이국 소녀들의 이름과, 벌써 아기 어머니 된 계집애들의 이름과, 가난한 이웃 사람들의 이름과, 비둘기, 강아지, 토끼, 노새, 노루, '프랑시스 잠', '라이너 마리아 릴케’ 이런 시인의 이름을 불러 봅니다.\",\n",
        "'이네들은 너무나 멀리 있습니다.',\n",
        "'별이 아스라이 멀듯이.',\n",
        "'어머님,',\n",
        "'그리고 당신은 멀리 북간도에 계십니다.',\n",
        "'나는 무엇인지 그리워',\n",
        "'이 많은 별빛이 내린 언덕 위에',\n",
        "'내 이름자를 써 보고',\n",
        "'흙으로 덮어 버리었습니다.',\n",
        "'딴은 밤을 새워 우는 벌레는',\n",
        "'부끄러운 이름을 슬퍼하는 까닭입니다.',\n",
        "'그러나 겨울이 지나고 나의 별에도 봄이 오면',\n",
        "'무덤 위에 파란 잔디가 피어나듯이',\n",
        "'내 이름자 묻힌 언덕 위에도',\n",
        "'자랑처럼 풀이 무성할 거외다.',]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjoJFnjNhwEG"
      },
      "source": [
        "tokens=[]\n",
        "for sentence in starry_night:\n",
        "    tokenezied_text = [token for token in sentence] \n",
        "    tokens.extend(tokenezied_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQME2eaoooVS"
      },
      "source": [
        "## 빈칸완성 과제 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSdZp0mfiBLQ"
      },
      "source": [
        "from collections import defaultdict\n",
        "vocab_dict= defaultdict(int)\n",
        "for token in tokens:\n",
        "  ##TODO##\n",
        "  '''\n",
        "  vocab_dict에 token을 key로 빈도수를 value로 채워넣으세요.\n",
        "  예시) vocab_dict={\"나\":3,\"그\":5,\"어\":3,...}\n",
        "  '''\n",
        "  vocab_dict[token] += 1\n",
        "  ##TODO##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHUDU4PoiOt_"
      },
      "source": [
        "sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pb-FfJmjoqz"
      },
      "source": [
        "vocab=[]\n",
        "for token,freq in sorted_vocab:\n",
        "  ##TODO##\n",
        "  '''\n",
        "  정렬된 sorted_vocab에서 빈도수가 2 이상인 token을 vocab에 append하여 vocab을 완성시키세요.\n",
        "  '''\n",
        "  if freq >= 2:\n",
        "    vocab.append(token)\n",
        "  ##TODO##\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35WIxjLZkJd4"
      },
      "source": [
        "answer=[' ','이',',','나','에','다','니','별','는', '하', '.', '아', '름', '을', '의', '과', '가', '은', '어', '지', '들', '리', '무', '머', '도', '까', '닭', '내', '러', '계', '습', '듯', '새', '랑', '시', \"'\", '멀', '그', '고', '위', '자', '로', '있', '속', '둘', '겨', '오', '요', '밤', '입', '사', '쓸', '경', '님', '운', '한', '마', '디', '불', '봅', '소', '런', '벌', '써', '기', '노', '스', '라', '너', '인', '워', '언', '덕']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1zCJNRiO5k"
      },
      "source": [
        "assert vocab==answer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}